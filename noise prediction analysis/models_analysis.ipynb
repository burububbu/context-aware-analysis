{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import utils.models_utils as models\n",
    "\n",
    "import const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "dataset = utils.load_dataset(10)\n",
    "\n",
    "# fit scalers\n",
    "# base set\n",
    "base_set_standard_scaler = StandardScaler().fit(dataset.x_train_base)\n",
    "base_set_minmax_scaler = MinMaxScaler().fit(dataset.x_train_base)\n",
    "\n",
    "# complete set\n",
    "complete_set_standard_scaler = StandardScaler().fit(dataset.x_train_complete)\n",
    "complete_set_minmax_scaler = MinMaxScaler().fit(dataset.x_train_complete)\n",
    "\n",
    "# sub set\n",
    "sub_set_standard_scaler = StandardScaler().fit(dataset.x_train_subset)\n",
    "sub_set_minmax_scaler = MinMaxScaler().fit(dataset.x_train_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn_params = {\n",
    "    'n_neighbors': [10, 20, 25, 30, 50, 60, 100, 150, 250],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get models with differents types of preprocessing\n",
    "base_results = models.train_models(KNeighborsRegressor(), dataset.x_train_base,\n",
    "                    dataset.y_train, dataset.x_test_base, dataset.y_test, knn_params, base_set_standard_scaler, base_set_minmax_scaler)\n",
    "base_results['set'] = 'base'\n",
    "\n",
    "base_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_results = models.train_models(KNeighborsRegressor(), dataset.x_train_subset,\n",
    "                    dataset.y_train, dataset.x_test_subset, dataset.y_test, knn_params, sub_set_standard_scaler, sub_set_minmax_scaler)\n",
    "                    \n",
    "sub_results['set'] = 'sub'\n",
    "\n",
    "sub_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results = models.train_models(KNeighborsRegressor(), dataset.x_train_complete,\n",
    "                    dataset.y_train, dataset.x_test_complete, dataset.y_test, knn_params, complete_set_standard_scaler, complete_set_minmax_scaler)\n",
    "\n",
    "complete_results['set'] = 'complete'\n",
    "\n",
    "complete_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results = base_results.append([sub_results, complete_results])\n",
    "knn_results['model'] = 'knn'\n",
    "\n",
    "utils.plot_scores(x='set', y='r2_test', hue='preprocessing',  data=knn_results, title='knn scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [200, 300, 500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get models with differents types of preprocessing\n",
    "base_results = models.train_models(RandomForestRegressor(), dataset.x_train_base,\n",
    "                    dataset.y_train, dataset.x_test_base, dataset.y_test, rf_params, base_set_standard_scaler, base_set_minmax_scaler)\n",
    "base_results['set'] = 'base'\n",
    "\n",
    "base_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_results = models.train_models(RandomForestRegressor(), dataset.x_train_subset,\n",
    "                    dataset.y_train, dataset.x_test_subset, dataset.y_test, rf_params, sub_set_standard_scaler, sub_set_minmax_scaler)\n",
    "sub_results['set'] = 'sub'\n",
    "\n",
    "sub_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results = models.train_models(RandomForestRegressor(), dataset.x_train_complete,\n",
    "                    dataset.y_train, dataset.x_test_complete, dataset.y_test, rf_params, complete_set_standard_scaler, complete_set_minmax_scaler)\n",
    "complete_results['set'] = 'complete'\n",
    "\n",
    "complete_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = base_results.append([sub_results, complete_results])\n",
    "\n",
    "rf_results['model'] = 'rf'\n",
    "\n",
    "utils.plot_scores(x='set', y='r2_test', hue='preprocessing',  data=rf_results, title='random forest scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_params = {\n",
    "    'early_stopping': [True, False],\n",
    "    'alpha': [10.0**-n for n in range(7)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get models with differents types of preprocessing\n",
    "base_results = models.train_models(SGDRegressor(), dataset.x_train_base,\n",
    "                    dataset.y_train, dataset.x_test_base, dataset.y_test, sgd_params, base_set_standard_scaler, base_set_minmax_scaler)\n",
    "base_results['set'] = 'base'\n",
    "\n",
    "base_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_results = models.train_models(SGDRegressor(), dataset.x_train_subset,\n",
    "                    dataset.y_train, dataset.x_test_subset, dataset.y_test, sgd_params, sub_set_standard_scaler, sub_set_minmax_scaler)\n",
    "sub_results['set'] = 'sub'\n",
    "\n",
    "sub_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results = models.train_models(SGDRegressor(), dataset.x_train_complete,\n",
    "                    dataset.y_train, dataset.x_test_complete, dataset.y_test, sgd_params, complete_set_standard_scaler, complete_set_minmax_scaler)\n",
    "complete_results['set'] = 'complete'\n",
    "\n",
    "complete_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_results = base_results.append([sub_results, complete_results])\n",
    "sgd_results['model'] = 'sgd'\n",
    "\n",
    "utils.plot_scores(x='set', y='r2_test', hue='preprocessing',  data=sgd_results[sgd_results['preprocessing'] !='no'], title='sgd scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base set (minmax scaled data)\n",
    "nn_params = {\n",
    "    \"hidden_sizes\": [5, 10, 20, 50, 100],\n",
    "    \"nums_layers\": [1, 2],\n",
    "    \"num_epochs\": [500, 1000],\n",
    "    \"batch_sizes\": [512],\n",
    "    \"learning_rates\": [0.1],\n",
    "    \"gamma\": [0.05], # lr decay\n",
    "    \"dropout\": [0, 0.2]\n",
    "}\n",
    "\n",
    "train_data_nn, test_data_nn = utils.get_nn_dataset(dataset.x_train_base, dataset.x_test_base, dataset.y_train, dataset.y_test ,scaler = base_set_minmax_scaler)\n",
    "\n",
    "base_nn_results = models.train_neural_nets(train_data_nn, test_data_nn, nn_params)\n",
    "base_nn_results['set'] = \"base\"\n",
    "base_nn_results['preprocessing'] = 'minmax'\n",
    "\n",
    "utils.res_to_csv(base_nn_results, f'{const.csv_results_folder}base_nn_results')\n",
    "\n",
    "# print three best results\n",
    "base_nn_results.sort_values('r2_test', ascending=False).head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub set (minmax scaled data)\n",
    "\n",
    "nn_params = {\n",
    "    \"hidden_sizes\": [15, 20, 50, 100, 200],\n",
    "    \"nums_layers\": [1, 2],\n",
    "    \"num_epochs\": [500, 1000],\n",
    "    \"batch_sizes\": [512],\n",
    "    \"learning_rates\": [0.1],\n",
    "    \"gamma\": [0.05], # lr decay\n",
    "    \"dropout\": [0, 0.2]\n",
    "}\n",
    "\n",
    "train_data_nn, test_data_nn = utils.get_nn_dataset(dataset.x_train_subset, dataset.x_test_subset, dataset.y_train, dataset.y_test, scaler=sub_set_minmax_scaler)\n",
    "\n",
    "sub_nn_results = models.train_neural_nets(train_data_nn, test_data_nn, nn_params)\n",
    "sub_nn_results['set'] = \"sub\"\n",
    "sub_nn_results['preprocessing'] = 'minmax'\n",
    "\n",
    "utils.res_to_csv(sub_nn_results, f'{const.csv_results_folder}subset_nn_results')\n",
    "\n",
    "sub_nn_results.sort_values('r2_test', ascending=False).head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete set (minmax scaled data)\n",
    "\n",
    "nn_params = {\n",
    "    \"hidden_sizes\": [20, 50, 100, 200],\n",
    "    \"nums_layers\": [1, 2],\n",
    "    \"num_epochs\": [500, 1000],\n",
    "    \"batch_sizes\": [512],\n",
    "    \"learning_rates\": [0.1],\n",
    "    \"gamma\": [0.05], # lr decay\n",
    "    \"dropout\": [0, 0.2]\n",
    "}\n",
    "\n",
    "train_data_nn, test_data_nn = utils.get_nn_dataset(dataset.x_train_complete, dataset.x_test_complete, dataset.y_train, dataset.y_test, scaler=complete_set_minmax_scaler)\n",
    "\n",
    "complete_nn_results = models.train_neural_nets(train_data_nn, test_data_nn, nn_params)\n",
    "complete_nn_results['set'] = \"complete\"\n",
    "complete_nn_results['preprocessing'] = 'minmax'\n",
    "\n",
    "utils.res_to_csv(complete_nn_results, f'{const.csv_results_folder}complete_nn_results')\n",
    "\n",
    "complete_nn_results.sort_values('r2_test', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results = base_nn_results.append([sub_nn_results, complete_nn_results]).sort_values('r2_test', ascending=False)\n",
    "nn_results['model'] = 'nn'\n",
    "\n",
    "# get best res for each set results\n",
    "nn_best_results = nn_results.groupby(['set', 'preprocessing']).head(1)\n",
    "\n",
    "utils.plot_scores('set', 'r2_test', 'preprocessing', nn_best_results, \"best neural net scores\")\n",
    "\n",
    "nn_best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = knn_results.append([rf_results, sgd_results, nn_best_results])\n",
    "\n",
    "total_best_results = total_results.sort_values(by=['r2_test'], ascending=False).groupby('model').head(1)\n",
    "\n",
    "utils.plot_scores('model', 'r2_test', None, total_best_results, 'mh')\n",
    "\n",
    "total_best_results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4127b179aa0bb31a49b22e75037871e1b5b9dc88b373cb715669f17b3e2dfe7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
